# CLI Integration Testing: Multi-Command Workflows

## Overview
Testing complex CLI workflows with multiple commands requires careful process management, environment isolation, and verification strategies. This document covers patterns from real-world tools like Cargo and Node.js CLI frameworks.

## Key Patterns and Techniques

### 1. Layered Setup Approach

**Pattern**: Divide setup into phases to avoid redundant expensive operations.

```
Setup Phase (runs once per suite)
├── Install dependencies
├── Compile project
└── Generate common artifacts

Test Phase (runs for each test)
├── Copy setup artifacts to isolated temp directory
├── Execute test commands
└── Verify results
```

**Benefits**:
- Eliminates repeating expensive operations like package installations
- Each test gets an isolated temporary directory
- Avoids conflicts between parallel tests
- Significant performance improvement for suites with multiple tests

**Real-world example**: The Pika CLI testing framework uses this pattern to setup build systems once and reuse across multiple command tests.

### 2. Async Process Control

**Pattern**: Long-running commands must be managed with async wrappers and signal detection.

```
Usage Pattern:
- Start process with runAsync()
- Wait for specific output signal (e.g., "Ready" message)
- Verify that server/process is operational
- Clean up process on test completion
```

**Key techniques**:
- Use `spawn()` to create child processes
- Listen for specific output patterns that indicate readiness
- Implement timeout handling for long-running operations
- Save `spawn_id` to switch between multiple concurrent processes

**Example**: When testing a debug server, wait for "Ready on http://localhost:PORT" output before making HTTP requests.

### 3. Directory and File Management

**Pattern**: Isolate file operations through temporary directories with setup artifacts.

```
Test Directory Structure:
test-dir/
├── setup-artifact-1
├── setup-artifact-2
└── test-generated-files
```

**Implementation details**:
- Create temporary directory per test via `t.TempDir()` (Go) or similar
- Copy setup artifacts to avoid conflicts
- Verify generated files at test completion
- Automatic cleanup after test (handled by test framework)

### 4. Multi-Channel Verification

**Pattern**: Verify CLI behavior through multiple channels beyond exit codes.

```
Verification strategies:
├── File system assertions
│   └── Check for expected build artifacts
├── HTTP requests
│   └── Validate API responses from servers started by CLI
├── Browser automation
│   └── Test UI generated by CLI tools using Playwright
└── Output assertions
    └── Verify stdout/stderr content
```

**Example**: For a build tool, verify:
1. Exit code is 0
2. Output contains expected log messages
3. Generated files exist with correct permissions
4. Hash/checksum of outputs matches expected

### 5. Environment Testing Matrix

**Pattern**: Test across multiple configurations without code duplication.

```yaml
Test Matrix:
- OS: [Windows, Linux, macOS]
- NodeVersion: [16.x, 18.x, 20.x]
- Bundler: [webpack, esbuild, vite]

For each combination:
- Run identical test pipeline
- Verify consistency across platforms
```

**Implementation**: Use CI matrix strategies (GitHub Actions, GitLab CI) to run the same test suite across configurations.

### 6. Port Management for Concurrent Tests

**Pattern**: Detect free ports dynamically to prevent conflicts.

```
Pattern:
- Instead of hardcoding port 3000
- Call OS function to find available port
- Start server on that port
- Pass port to test assertions
```

**Benefits**:
- Enables parallel test execution
- Prevents "address already in use" errors
- Works across different environments

## Testing Frameworks and Tools

### Judo Framework (YAML-driven)
- **Purpose**: Command execution and assertion framework
- **Features**:
  - Execute commands
  - Respond to stdin
  - Assert exit codes
  - Check stdout/stderr for strings
  - Multiple spawn support for multi-process workflows
- **Documentation**: [GitHub - intuit/judo](https://github.com/intuit/judo)

### Aruba (Ruby)
- **Extends**: Cucumber, RSpec, minitest
- **Strengths**: Excellent for testing CLI applications in any language
- **Features**: Process management, file assertions, output verification

### Bats (Bash)
- **Purpose**: TAP-compliant testing for Bash scripts
- **Strengths**: Fast, no dependencies, works for any CLI
- **Use case**: Testing shell-based CLI tools

### Expect
- **Pattern**: `spawn` and `expect` for interactive CLIs
- **Multi-process**: Set `spawn_id` to switch between multiple processes
- **Example**: Automate responses to prompts and verify output

## Real-World Examples

### Cargo (Rust Package Manager)
- Uses integration tests in `/tests` directory
- Tests built binary with `assert_cmd` crate
- Verifies file system side effects with `assert_fs`
- Tests build artifacts and outputs

### Pika Web Framework
- Layered setup: Shared build system setup
- Multi-bundler testing: Test against webpack, esbuild, vite
- Cross-platform CI matrix: Windows, Linux, macOS
- Free port detection for concurrent dev servers
- File verification and HTTP assertions

## Anti-Patterns to Avoid

1. **No isolation between tests**: Tests that depend on shared state fail unpredictably
2. **Hardcoded ports**: Prevents parallel execution
3. **Expensive repeated setup**: Each test re-installs dependencies
4. **Over-mocking**: Don't mock what you should actually test
5. **Ignoring signals**: Tests hang waiting for processes that never respond
6. **No cleanup**: Temporary files accumulate and cause interference

## Best Practices

1. **Keep fixtures minimal**: One well-defined purpose per fixture
2. **Use descriptive names**: "test_build_with_multiple_entry_points" > "test_2"
3. **Clear error messages**: Include context when assertions fail
4. **Parallel-safe defaults**: Enable parallel execution from the start
5. **Document signal expectations**: What output indicates success?
6. **Monitor cleanup**: Ensure temp directories are always removed
7. **Test critical paths**: Focus on user-observable behavior

## References

- [DEV Community: How we wrote our CLI integration tests](https://dev.to/florianrappl/how-we-wrote-our-cli-integration-tests-53i3)
- [Real Python: CLI Testing Techniques](https://realpython.com/python-cli-testing/)
- [Intuit Judo Framework](https://github.com/intuit/judo)
- [Painless CLI Integration Testing](https://dev.to/valorsoftware/painless-cli-integration-testing-1bd7)
- [Medium: Integration tests on Node.js CLI](https://medium.com/@zorrodg/integration-tests-on-node-js-cli-part-1-why-and-how-fa5b1ba552fe)
